# -*- coding: utf-8 -*-
"""Complete-RAG-PIPELINE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YtEBP1MgtBozg6imU_NxEdF9gwTo4nYT
"""

!pip install langchain
!pip install openai
!pip install moviepy

!pip install whisper
!pip install git+https://github.com/openai/whisper.git #to leverage the latest updates/commands.

!pip install chromadb

from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb
import whisper
from moviepy.editor import VideoFileClip
from openai import OpenAI

!pip install gradio

!pip install sentence_transformers

from sentence_transformers import SentenceTransformer
model= SentenceTransformer('all-MiniLM-L6-v2') #all-mpnet-base-v2

import gradio

#converting video to audio file
video_filename="WhatisPromptTuning_.mp4"
audio_filename="audio.mp3"
clip= VideoFileClip(video_filename)
clip.audio.write_audiofile(audio_filename)

model1= whisper.load_model("base")
result=model1.transcribe(audio_filename)
text=result["text"]

text_splitter= RecursiveCharacterTextSplitter(chunk_size=300,
                                              chunk_overlap=80)
chunks=text_splitter.split_text(text)

len(chunks)

chunks

type(chunks)

#initialise chromadb
db=chromadb.Client()
collection = db.create_collection(name="my_collection")


# Check if a collection named "test" already exists
"""if "test" in db.list_collections():
    print("Collection 'test' already exists. Using existing collection.")
    collection = db.get_collection("test")  # Get the existing collection
else:
    collection = db.create_collection("test") #created the "test" named collection."""

#adding and embedding the chunks using chromadb.
collection.add(
    ids=[str(i) for i in range(len(chunks))], #important to generated the unique id for each chunk
    documents=chunks,
    metadatas=[{"source": f"source{i}"} for i in range(len(chunks))]
)

collection

type(collection)

query_emb= input("Enter your query:")#asking the question as well as embedding the asked question.
query_embedding=model.encode(query_emb).tolist()

"""results= collection.query(
    query_embeddings=[query_embedding],
    n_results=4) #n_results == k (top results)"""

while True:
  query = input("Enter the query ('quit' to exit): ")
  if query.lower() == 'quit':
    print("Hope you enjoyed it!!")
    break  # Exit the loop if the user enters 'quit'
  query_emb= [query]
  results= collection.query(
      query_texts=query_emb,
      n_results=2)
  for i in range(len(results['documents'][0])):
      print(f"The answer to the query is: {results['documents'][0][i]}")
      #print(f"The distance is: {results['distances'][0][i]}")
      print()

from google.colab import userdata
API_KEY=userdata.get('API_KEY_MY')

import os
from openai import OpenAI
os.environ["OPENAI_API_KEY"] = API_KEY
def generate_response(prompt):
    client = OpenAI()
    chat_completion = client.chat.completions.create(
    messages=[{"role": "system", "content": "You are a helpful assistant."},
     {"role": "user", "content": prompt}],
    model="gpt-3.5-turbo",
    )
    return chat_completion.choices[0].message.content

prefix="Answer the query specific to the given context and don't add anything outside of it."

prompt=f'''{prefix}\n\n\
Question: {query_emb}\n\
Context: {results}\n\
Answer:'''

print(generate_response(prompt))

"""import os
import openai
from openai import OpenAI
os.environ["OPENAI_API_KEY"] = API_KEY

# Function to generate response from GPT-3.5 Turbo
def get_response(prompt, model="gpt-3.5-turbo"):
  client = OpenAI()
  query_emb= input("Enter the query")#asking the question as well as embedding the asked question.
  results= collection.query(
      query_embeddings=query_emb,
      n_results=4) #n_results == k (top results)
  response = client.chat.completions.create(
    model=model,
    messages=[{"role": "system", "content": "You are a helpful assistant."},
     {"role": "user", "content": f"{prompt}"}
              ]
  )
  return response.choices[0].message.content
  """

"""import gradio as gr
def chat_with_bot(video,query):
  response=generate_response(prompt)
  return response

interface = gr.Interface(
      fn=chat_with_bot,
      inputs=[gr.Video(label="Upload Video for Reference!"),gr.Textbox(label="Enter your Query!")],
      outputs=[gr.Textbox(label="Answer to the query!")],
      title="GPT-3.5 Turbo Chatbot",
      description= "",
      )
interface.launch(debug=True)"""
import gradio as gr
# Define the chatbot function to interact with the user
def chat_with_bot(video,query):
    return generate_response(query)

# Create the Gradio interface
iface = gr.Interface(
    fn=chat_with_bot,
    inputs=[gr.Video(label="Upload Video for Reference!"),gr.Textbox(label="Enter your Query!")],
    outputs=[gr.Textbox(label="Answer to the query!")],
    title="GPT-3.5 Turbo Chatbot",
    description="Ask anything and get responses from GPT-3.5 Turbo."
)

# Launch the interface
iface.launch(share=True)

